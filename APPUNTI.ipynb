{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Basic Statistics You NEED to Know for Data Science](https://towardsdatascience.com/basic-statistics-you-need-to-know-for-data-science-1fdd290f59b5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Recommendation System Series Part 1](https://towardsdatascience.com/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [90% of Warren Buffettâ€™s will follows this strategy](https://medium.com/the-post-grad-survival-guide/its-surprisingly-easy-to-make-money-in-the-stock-market-here-s-how-19e9cf711b23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importare e manipolare un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"daily_GS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec = pd.read_csv(\"/content/churn.csv\", usecols=['Gender', 'Age', 'Tenure', 'Balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading a part of the dataframe\n",
    "df_partial = pd.read_csv(\"/content/churn.csv\", nrows=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of rows in the sample\n",
    "df_sample = df.sample(n=1000)\n",
    "#The ratio of the sample size to the whole dataframe size\n",
    "df_sample2 = df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(file_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_GS_CSV['index'] = range(0, len(dataset_GS_CSV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_index = np.random.randint(100, size=20)\n",
    "#use these indices to change some values as np.nan (missing value)\n",
    "df_spec.loc[missing_index, ['Gender','Age']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### manipolazione delle date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn date column into datetime\n",
    "df.Date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Weekday\n",
    "df['weekday'] = df.Date.dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Holidays\n",
    "!pip install holidays\n",
    "import holidays\n",
    "california_holidays = holidays.UnitedStates(state='CA')\n",
    "print(california_holidays.get(df.Date[0]))\n",
    "print(california_holidays)\n",
    "print(california_holidays.get('2020-12-01'))\n",
    "#to create a holiday feature for dataset\n",
    "df['holidays'] = df.Date.apply(lambda date: california_holidays.get(date))\n",
    "df.holidays = df.holidays.fillna('No holiday') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipolazione Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratto da [qui](https://towardsdatascience.com/30-examples-to-master-pandas-f8a2da751fa4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ricampionamento del dataset\n",
    "average_per_month = df.set_index('Date').resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.reindex(['date','close','open','high','low','volume'], axis=1)\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.set_index('date').sort_values(['date'] , ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "df2.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.notnull(df2).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(df2).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.fillna(0).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.fillna(df2.mean()).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values\n",
    "avg = df['Balance'].mean()\n",
    "df['Balance'].fillna(value=avg, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolazione\n",
    "dataset = df.replace(to_replace = 0.00, value = np.nan).interpolate(method='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering\n",
    "df[df['column_name'] == 0]\n",
    "df[df['column_name'] == \"hello\"]\n",
    "france_churn = df[(df.Geography == 'France') & (df.Exited == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where function: to set the balance to 0 for customers who belong to a group that is less than 6.\n",
    "df_new['Balance'] = df_new['Balance'].where(df_new['Group'] >= 6, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_df['anomalia'] = np.where(signals_df.inlier_mask == True, 0, 1)\n",
    "\n",
    "signals_df.loc[signals_df['inlier_mask'] == True]\n",
    "\n",
    "signals_df['inlier_mask'] = signals_df['inlier_mask'].map({True: 0, False: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering based on strings\n",
    "df_new[df_new.Names.str.startswith('Mi')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The replace function can be used to replace values in a dataframe.\n",
    "df_new['Group'].replace(0, 'B1')\n",
    "#The first parameter is the value to be replaced and the second one is the new value.\n",
    "#use a dictionary to do multiple replacements.\n",
    "df_new['Group'].replace({1: 'B1',2:'B2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the display options\n",
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cancella i duplicati\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unisce due dataframe in orrizzontale\n",
    "df = pd.merge(df, df1, on='Date', how='left')\n",
    "df.join(lookup_dataframe, on='column_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raggruppamenti\n",
    "df.groupby(['column1', 'column2']).sum()\n",
    "df.groupby(['column1', 'column2']).count()\n",
    "df.groupby(['column1', 'column2']).agg({'column1': 'sum', 'column2': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ricampionamento\n",
    "dataset = dataset.resample('W').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variazioni percentuali\n",
    "dataset['pct_change'] = dataset['Close'].pct_change() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some values with a different color based on a condition.\n",
    "def color_negative_values(val):\n",
    "    color = 'red' if val < 0 else 'black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "df.style.applymap(color_negative_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same, background color\n",
    "def color_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: lightblue' if v else '' for v in    is_max]\n",
    "\n",
    "df.style.apply(color_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply this function to rows by setting axis parameter as 1.\n",
    "df.style.apply(color_max, axis=1)\n",
    "\n",
    "#combine different style functions by chain operations.\n",
    "df.style.applymap(color_negative_values).apply(color_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [altre possibilitÃ  di stile](https://towardsdatascience.com/style-your-pandas-dataframes-814e6a078c6d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [How to handle time series data.](https://towardsdatascience.com/20-points-to-master-pandas-time-series-analysis-f90155ee0e8a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicazioni Finanziarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlazioni fra due titoli\n",
    "corr = df_corr['close_pct'].corr(df_corr['Close_DX_pct'])\n",
    "\n",
    "#RischiositÃ  di un titolo\n",
    "var = df_corr['Close_DX_pct'].var()\n",
    "math.sqrt(var)\n",
    "\n",
    "#Rendimento atteso\n",
    "rendimento_atteso = df_corr['Close_DX_pct'].mean()\n",
    "rendimento_atteso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the short window and long windows\n",
    "short_window = 50\n",
    "long_window = 100\n",
    "# Generate the short and long moving averages (50 and 100 days, respectively)\n",
    "signals_df['SMA50'] = signals_df['Close'].rolling(window=short_window).mean()\n",
    "signals_df['SMA100'] = signals_df['Close'].rolling(window=long_window).mean()\n",
    "signals_df['Signal'] = 0.0\n",
    "# Generate the trading signal 0 or 1,\n",
    "# where 0 is when the SMA50 is under the SMA100, and\n",
    "# where 1 is when the SMA50 is higher (or crosses over) the SMA100\n",
    "signals_df['Signal'][short_window:] = np.where(signals_df['SMA50'][short_window:] > signals_df['SMA100'][short_window:], 1.0, 0.0)\n",
    "# Calculate the points in time at which a position should be taken, 1 or -1\n",
    "signals_df['Entry/Exit'] = signals_df['Signal'].diff()\n",
    "# Print the DataFrame\n",
    "signals_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso delle liste in Python\n",
    "First of all, the basic syntax: every list comprehension should abide by the template as following:  \n",
    "my_list=[ expression for item in iterable (if condition) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'a', 'n', 'g', ' ', 'Z', 'h', 'o', 'u']\n"
     ]
    }
   ],
   "source": [
    "#Level 1: Just Replace For-loops\n",
    "full_name = \"Yang Zhou\"\n",
    "characters = [char for char in full_name]\n",
    "print(characters)\n",
    "# ['Y', 'a', 'n', 'g', ' ', 'Z', 'h', 'o', 'u']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'a', 'n', 'g', ' ', 'Z', 'h', 'o', 'u']\n"
     ]
    }
   ],
   "source": [
    "#The above program is equal to:\n",
    "full_name = \"Yang Zhou\"\n",
    "characters = []\n",
    "for char in full_name:\n",
    "    characters.append(char)\n",
    "print(characters)\n",
    "# ['Y', 'a', 'n', 'g', ' ', 'Z', 'h', 'o', 'u']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 99, 33]\n"
     ]
    }
   ],
   "source": [
    "Matrix = [[2, 1, 5],\n",
    "          [5, 99, 0],\n",
    "          [33, 2, 4]]\n",
    "row_max = [max(row) for row in Matrix]\n",
    "print(row_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yang'] ['Yang', 'Tom', 'tom'] ['tom']\n"
     ]
    }
   ],
   "source": [
    "#Level 2: Use the If Condition Smartly\n",
    "Genius = [\"Yang\", \"Tom\", \"Jerry\", \"Jack\", \"tom\", \"yang\"]\n",
    "L1 = [name for name in Genius if name.startswith('Y')]\n",
    "L2 = [name for name in Genius if name.startswith('Y') or len(name) < 4]\n",
    "L3 = [name for name in Genius if len(name) < 4 and name.islower()]\n",
    "print(L1, L2, L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jerry', 'Jack', 'Tom', 'Yang']\n"
     ]
    }
   ],
   "source": [
    "#Level 3: Use a More Complex Expression\n",
    "Genius = [\"Jerry\", \"Jack\", \"tom\", \"yang\"]\n",
    "L1 = [name.capitalize() for name in Genius]\n",
    "print(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Not Genius', 'Not Genius', 'Not Genius', 'yang']\n"
     ]
    }
   ],
   "source": [
    "Genius = [\"Jerry\", \"Jack\", \"tom\", \"yang\"]\n",
    "L1 = [name if name.startswith('y') else 'Not Genius' for name in Genius]\n",
    "print(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J', 'e', 'r', 'r', 'y', 'J', 'a', 'c', 'k', 't', 'o', 'm', 'y', 'a', 'n', 'g']\n"
     ]
    }
   ],
   "source": [
    "#Level 4: Use Nested For-Loops to Handle Nested Iterables\n",
    "Genius = [\"Jerry\", \"Jack\", \"tom\", \"yang\"]\n",
    "L1 = [char for name in Genius for char in name]\n",
    "print(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J', 'e', 'r', 'r', 'y', 'J', 'a', 'c', 'k', 't', 'o', 'm', 'y', 'a', 'n', 'g']\n"
     ]
    }
   ],
   "source": [
    "#The above program is equal to:\n",
    "Genius = [\"Jerry\", \"Jack\", \"tom\", \"yang\"]\n",
    "L1 = []\n",
    "for name in Genius:\n",
    "    for char in name:\n",
    "        L1.append(char)\n",
    "print(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'o', 'm']\n"
     ]
    }
   ],
   "source": [
    "#In addition, we can add the optional if conditions after any for-loops:\n",
    "Genius = [\"Jerry\", \"Jack\", \"tom\", \"yang\"]\n",
    "L1 = [char for name in Genius if len(name) < 4 for char in name]\n",
    "print(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom']\n",
      "['tom']\n"
     ]
    }
   ],
   "source": [
    "#Level 5: Avoid Higher Order Functions for Readability\n",
    "Genius = [\"Jerry\", \"Jack\", \"tom\", \"yang\"]\n",
    "L1 = filter(lambda a: len(a) < 4, Genius) #<- Itâ€™s a good habit to always use the list comprehension instead of higher order functions such as map() , filter() and so on. \n",
    "print(list(L1))\n",
    "\n",
    "L2 = [a for a in Genius if len(a) < 4]\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8697440\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "#Level 6: Use Generator Expressions to Reduce Memory Costs\n",
    "large_list = [x for x in range(1_000_000)]\n",
    "large_list_g = (x for x in range(1_000_000))\n",
    "print(large_list.__sizeof__())\n",
    "print(large_list_g.__sizeof__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Level 7: Understand the Philosophy Behind the List Comprehension  \n",
    "Il motivo intuitivo per utilizzare le List Comprehension Ã¨ rendere il nostro codice piÃ¹ pulito ed elegante. Inoltre, Ã¨ una buona pratica del paradigma di programmazione funzionale. Una delle filosofie della programmazione funzionale consiste nell'evitare flussi di controllo. La comprensione dell'elenco puÃ² spostare l'attenzione dei programmatori dal flusso di controllo alla raccolta dei dati stessa. In altre parole, Ã¨ un passaggio mentale dal pensare a come funziona un ciclo for a quello che Ã¨ l'elenco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi esplorativa e creazione grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"daily_GS.csv\")\n",
    "df3['index'] = df3.index\n",
    "df3 = df3.reindex(['index','date','close','open','high','low','volume'], axis=1)\n",
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizzazione dei dati mancanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df3.isnull(), yticklabels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi di Regressione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"index\", y=\"close\", data=df3, size = 4, aspect = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Andamento di una singola variabile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df3.close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice a correlazione\n",
    "Fornisce un utile riepilogo grafico che evidenzia le correllazioni lineari tra le caratteristiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df3[['close','open','low','high','volume']].corr(), annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un altro tipo di correlazione\n",
    "In questo caso la maggior parte dei voli si svolge nei mesi di luglio e agosto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sns.load_dataset('flights')\n",
    "f1 = flights.pivot_table(values='passengers',index='month',columns='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice a dispersione\n",
    "Consente di rappresentare le correlazioni a coppie fra le diverse caratteristiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df3[['close','open','low','high','volume']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafico a barre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.bar(birth_df, x='holidays', y='Births')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzioni di utilitÃ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['close'].value_counts(dropna=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['close']\n",
    "y = df['open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom plot\n",
    "logX = np.log1p(x) # no NaNs after this operation\n",
    "plt.plot(logX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## histogram\n",
    "plt.hist(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=10, marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-8,-7, -6,-5 ,-4, -3, -2, -1.5, -1, -0.5, 1,2  ,3  ,4  ,5  ,6  ,7  ,8  ,9  ,10  ])\n",
    "y = np.array([1,1.5,1.6,1.8,2.0,2.1,2.3,2.5,2.4,2.5,2.6 ,2.8 ,3.0 ,3.2 ,3.3 ,4.0 ,5.0 ,6.0 ,8.0 ,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x+y\n",
    "plt.scatter(x,z)\n",
    "plt.scatter(x,y)\n",
    "plt.scatter(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean().sort_values().plot(style='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.diag(range(15))\n",
    "plt.matshow(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "**And visualizations are our art tools**\n",
    "* Histograms\n",
    "* Plots \n",
    "* Statistics\n",
    "* Scatter plots\n",
    "* Correlation plots\n",
    "* and more  \n",
    "\n",
    "Mai trarre conclusioni basate da un singolo diagramma, se si formula un'ipotesi, provarli su differenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top plot\n",
    "plt.plot(x, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.1, 0.3, 1,2  ,3  ,4  ,5  ,6  ,7  ,8  ,9  ,10  ])\n",
    "y = np.array([ 1, 2, 2.6 ,2.8 ,3.0 ,3.2 ,3.3 ,4.0 ,5.0 ,6.0 ,8.0 ,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom plot\n",
    "logX = np.log1p(x) # no NaNs after this operation\n",
    "plt.plot(logX, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is a target vector\n",
    "plt.scatter(x, y, c = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deviazione standard\n",
    "*sigma = sqrt( mean( abs(x - x.mean() )**2 ))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(data, stdev):\n",
    "  N = len(data)\n",
    "  return data + np.random.randn(N) * stdev\n",
    "  \n",
    "# sigma is a given std. dev. for Gaussian distribution\n",
    "plt.scatter(jitter(x, sigma), jitter(y, sigma), c = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['close'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementazione LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style='ticks', context='talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01', end='2010-01-01').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = web.DataReader(\"GS\", data_source='yahoo', start='2017-01-01').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock['Adj Close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure () \n",
    "plt.plot (df_stock [\"Volume\"]) \n",
    "plt.title ('Cronologia volume stock GE') \n",
    "plt.ylabel ('Volume') \n",
    "plt.xlabel ('Giorni') \n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"verifica se sono presenti valori nulli \\n\", df_stock.isna ().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = [\"Open\",\"High\",\"Low\",\"Volume\",\"Adj Close\"]\n",
    "df_train, df_test = train_test_split(df_stock, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "print(\"Train and Test size\", len(df_train), len(df_test))\n",
    "# scale the feature MinMax, build array\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "TIME_STEPS = 10\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def build_timeseries(mat, y_col_index):\n",
    "    # y_col_index Ã¨ l'indice della colonna che fungerebbe da colonna di output\n",
    "    # il numero totale di campioni di serie storiche sarebbe len (mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    \n",
    "    for i in tqdm.notebook.tqdm(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t, y_t = build_timeseries(x_train, 4)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "\n",
    "x_temp, y_temp = build_timeseries(x_test, 4)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras import optimizers\n",
    "\n",
    "lstm = LSTM(100, \n",
    "            batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]), \n",
    "            dropout=0.0, \n",
    "            recurrent_dropout=0.0, \n",
    "            stateful=True, \n",
    "            kernel_initializer='random_uniform')\n",
    "\n",
    "optimizer = optimizers.RMSprop(lr=0.001, rho=0.9)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(lstm)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'logs'\n",
    "EPOCHS = 100\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(os.path.join(OUTPUT_PATH, 'your_log_name' + '.log'), append=True)\n",
    "\n",
    "history = model.fit(x_t, y_t, epochs=EPOCHS, verbose=2, batch_size=BATCH_SIZE,\n",
    "                    shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                    trim_dataset(y_val, BATCH_SIZE)), callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up your validation to mimic the train/test split of the competition.\n",
    "#### Major data splitting strategies employed in competitions: \n",
    "* Random split\n",
    "* time-based split\n",
    "* ID-based split\n",
    "* and their combinations. \n",
    "\n",
    "scikit-learn - [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final project advice #2\n",
    "A good exercise is to reproduce previous_value_benchmark. As the name suggest - in this benchmark for the each shop/item pair our predictions are just monthly sales from the previous month, i.e. October 2015.  \n",
    "\n",
    "The most important step at reproducing this score is correctly aggregating daily data and constructing monthly sales data frame. You need to get lagged values, fill NaNs with zeros and clip the values into [0,20] range. If you do it correctly, you'll get precisely 1.16777 on the public leaderboard.  \n",
    "\n",
    "Generating features like this is a necessary basis for more complex models. Also, if you decide to fit some model, don't forget to clip the target into [0,20] range, it makes a big difference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indice di correlazione di Pearson\n",
    "https://it.wikipedia.org/wiki/Indice_di_correlazione_di_Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "counts = np.array([18, 3, 15, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts/counts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics optimization  \n",
    "The metric or target metric is a function which we want to use to evaluate the quality of our model. For example, for a classification task, we may want to maximize accuracy of our predictions, how frequently the model outputs the correct label.  \n",
    "The loss function is a function that our model optimizes and uses to evaluate the solution, and the target metric is how we want the solution to be evaluated.\n",
    "## 1) Regression  \n",
    "* $ MSE =  \\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N}  (y_{i} - \\hat{y}_{i})^2 $ \n",
    "* $ RMSE =  \\sqrt { \\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N}  (y_{i} - \\hat{y}_{i})^2 } = \\sqrt {MSE} $\n",
    "* $ R-squared = 1 - \\frac {{\\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N}  (y_{i} - \\hat{y}_{i})^2}}{{\\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N}  (y_{i} - \\bar{y})^2}} = 1 -  \\frac {{MSE}}{{\\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N}  (y_{i} - \\bar{y})^2}} $ dove $  \\bar{y} = \\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N}  y_{i} $\n",
    "* $ MAE =  \\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N}  \\big|y_{i} - \\hat{y}_{i}\\big| $\n",
    "* $ (R)MSPE = \\frac {{ 100\\% }}{{N}}\\sum\\limits_{i=1}^{N}  \\big( \\frac {{y_{i} - \\hat{y}_{i}}}{{y_{i}}}   \\big)^2 $\n",
    "* $ MAPE = \\frac {{ 100\\% }}{{N}}\\sum\\limits_{i=1}^{N}  \\big| \\frac {{y_{i} - \\hat{y}_{i}}}{{y_{i}}}   \\big|^2 $\n",
    "* $ (R)MSLE = \\sqrt { \\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N}  \\big(log(y_{i} + 1) - log(\\hat{y}_{i} + 1)\\big)^2 } $\n",
    "\n",
    "#### MAE vs MSE\n",
    "* Do you have outliers in the data? Use MAE\n",
    "* Are you sure they are outliers? Use MAE\n",
    "* Oppure sono solo valori inaspettati di cui dovremmo ancora preoccuparci? Use MSE  \n",
    "\n",
    "#### Conclusion\n",
    "* MSE, RMSE, R-squared, are the same from optimization perspective\n",
    "* MAE, robust to outliers\n",
    "\n",
    "## 2) Classification\n",
    "* $ Accurancy = \\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N}  \\big[\\hat{y}_{i} = y_{i}\\big] $ \n",
    "* $ LogLoss (binary) = - \\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N} y_{i} log(\\hat{y}_{i}) + (1 - y_{i}) log(1 - \\hat{y}_{i}) $\n",
    "* $ LogLoss (multiclass = - \\frac {{1}}{{N}}\\sum\\limits_{i=1}^{N} \\sum\\limits_{i=1}^{L} y_{il} log(\\hat{y}_{il})) $\n",
    "* AUC \n",
    "* $ Cohen's (Quadratic weighted) Kappa = 1 - \\frac {{1 - accuracy}}{{1 - p_{e}}} $  \n",
    "$ p_{e} = \\frac{1}{N^2} \\sum\\limits_{k} n_{k1} n_{k2} $ = what accuracy would be on average, if we randomly permute our predictions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ 1/N \\sum\\limits_{i=1}^{n}\\gamma_i h_i(x) $$\n",
    "$$\\bar{y} = a + b\\bar{x}$$\n",
    "$$SSResid = \\sum (y - \\hat{y})^2 = \\sum y^2 - a\\sum y - b \\sum xy$$\n",
    "$$s_{a + bx} = s_{e} \\sqrt {1 + \\frac {1}{n} + \\frac {(x - \\bar{x})^2}{{SS}_{xx}}}$$\n",
    "$$se(y - \\hat{y}) = s_{e} \\sqrt {1 + \\frac {1}{n} + \\frac {(x - \\bar{x})^2}{{SS}_{xx}}}$$\n",
    "$$\\bar{y} = a + b\\bar{x}$$\n",
    "$$E(y) = \\alpha + \\beta{x}$$\n",
    "$$\\mu_{\\bar{x_{1}} - \\bar{x_{2}}} = \\mu_{1} - \\mu_{2}$$\n",
    "$r=\\frac{\\sum\\limits_{i=1}^{n} (X_i - \\bar{X}) (Y_i - \\bar{Y}) }{\\sqrt {\\sum\\limits_{i=1}^{n} (X_i - \\bar{X})^2} \\sqrt {\\sum\\limits_{i=1}^{n} (Y_i - \\bar{Y})^2} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Encoding\n",
    "The general idea of this technique is to add new variables based on some feature to get where we started.   \n",
    "In simplest case, we encode each level of categorical variable with corresponding target mean.  \n",
    "* Likelihood = $  \\frac {Goods} {Goods + Bads} = mean(target)$\n",
    "* Weight of Evidence = $ln (\\frac {Goods}{Bads} * 100)  $ Another popular option is to take initial logarithm of this value.\n",
    "* Count = $ Goods = sum(target)  $\n",
    "* Diff = $ Goods - Bads  $  \n",
    "\n",
    "Just be careful and use correct validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "* Insert (or increase rate of) Dropout layers inside NN. Yes, Dropout is a known way to regularize model via dropping some randomly selected features at every step.\n",
    "* Add (or increase) Weight Decay. Weight Decay is essentially the same thing as L2 Regularization.\n",
    "* Reduce number of parameters (e.g. remove some layers) Reducing number of parameters make model less flexible and harder to overfit  \n",
    "\n",
    "A powerful regularizations is static dropconnect: usare un numero molto alto di unitÃ  nel secondo layer e fare dropout nel layer successivo:   \n",
    "in 64 -> 4096 -> 128 -> out 32\n",
    "\n",
    "### Hyperparameters that are first to tune in sklearn's RandomForest\n",
    "n_estimators, max_depth, min_samples_split Yes! These parameters are important. The first one should just be sufficiently large, you do not actually need to tune it.\n",
    "#### PiÃ¹ alto Ã¨ il peso della regolarizzazione, piÃ¹ il modello fa fatica ad imparare qualcosa.\n",
    "L1 / L2 / L1 + L2 -- provare ogni combinazione  \n",
    "L1 puÃ² essere usato per la selezione delle caratteristiche piÃ¹ rilevanti  \n",
    "\n",
    "Do not spend too much time on tuning hyperparameters, especially when the competition has only begun. You cannot win a competition by tuning parameters. Appropriate features, hacks, leaks, and insights will give you much more than carefully tuned model built on default features.  \n",
    "\n",
    "[Tuning the hyper-parameters of an estimator (sklearn)](http://scikit-learn.org/stable/modules/grid_search.html)  \n",
    "[Optimizing hyperparameters with hyperopt](http://fastml.com/optimizing-hyperparams-with-hyperopt/)  \n",
    "[Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and tricks\n",
    "Spent time to do some exploratory data analysis (EDA). Plot histograms of variables.  \n",
    "Check that a features looks similar between train and test.\n",
    "#### Questo Ã¨ l'apice dell'apprendimento automatico in questo momento\n",
    "* You need scikit learn and XGBoost, LightGBM  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/modeling.png)  \n",
    "\n",
    "# If you see it like a game, you never need to work for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Interactions\n",
    "Frequent operations for feature interaction:\n",
    "* Multiplication\n",
    "* Sum\n",
    "* Diff\n",
    "* Division\n",
    "\n",
    "Permette di creare nuove caratteristiche combinando in qualche modo quelle giaÃ  esistenti. A volta si ottiene un miglioramento di predizione.  \n",
    "Feature random forests over them and select several most important features:\n",
    "* 1. Fit Random Forest\n",
    "* 2. Get feature importances\n",
    "* 3. Select a few most important\n",
    "\n",
    "Construct categorical features from decision trees (es. random forest). The index of the object's leaf can be used as a value for a new categorical feature. In xgboost, also support to why a parameter breed leaf in predict method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tSNE is widely used in exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF\n",
    "Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced features II > Additional Materials and Links\n",
    "Matrix Factorization:  \n",
    "* [Overview of Matrix Decomposition methods (sklearn)](http://scikit-learn.org/stable/modules/decomposition.html)  \n",
    "\n",
    "t-SNE:\n",
    "* [Multicore t-SNE implementation](https://github.com/DmitryUlyanov/Multicore-TSNE)\n",
    "* [Comparison of Manifold Learning methods (sklearn)](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html)\n",
    "* [How to Use t-SNE Effectively (distill.pub blog)](https://distill.pub/2016/misread-tsne/)\n",
    "* [tSNE homepage (Laurens van der Maaten)](https://lvdmaaten.github.io/tsne/)\n",
    "* [Example: tSNE with different perplexities (sklearn)](http://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py)  \n",
    "\n",
    "Interactions:\n",
    "* [Facebook Research's paper about extracting categorical features from trees](https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/)\n",
    "* [Example: Feature transformations with ensembles of trees (sklearn)](http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling\n",
    "* Combining many different machine learning models in order to get a more powerful prediction.\n",
    "\n",
    "There are various ensemble methods.\n",
    "* Averaging (or blending)\n",
    "* Weighted averaging\n",
    "* Conditional averaging\n",
    "* Bagging\n",
    "* Boosting (Xgboost, Lightgbm)\n",
    "* Stacking\n",
    "* StackNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def check_for_stationarity(X, cutoff=0.01):\n",
    "    # We must observe significant p-value to convince ourselves that the series is stationary\n",
    "    pvalue = round(adfuller(X)[1], 4)\n",
    "    if pvalue < cutoff:\n",
    "        print('p-value = ' + str(pvalue) + ' The series ' + X.name +' is likely stationary.')\n",
    "        return True\n",
    "    else:\n",
    "        print('p-value = ' + str(pvalue) + ' The series ' + X.name +' is likely non-stationary.')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversione fogli Word da .doc a .docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from win32com import client as wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = wc.Dispatch('Word.Application')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=w.Documents.Open(\"C:/Users/ASUS/docx/Ten_Deep_Learning_Concepts.doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.SaveAs(\"C:/Users/ASUS/docx/Ten_Deep_Learning_Concepts.docx\",16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
